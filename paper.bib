@inproceedings{bm25,
	title        = {Okapi at {TREC}-3.},
	author       = {Robertson, Stephen and Walker, Steve and Jones, Susan and Hancock-Beaulieu, Micheline and Gatford, Mike},
	year         = 1995,
	month        = {01},
	booktitle    = {TREC},
	address      = {Gaithersburg, MD, US},
	url          = {https://trec.nist.gov/pubs/trec3/t3\_proceedings.html}
}
@incollection{alexnet,
	title        = {ImageNet Classification with Deep Convolutional Neural Networks},
	author       = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
	year         = 2012,
	booktitle    = {Advances in Neural Information Processing Systems 25},
	publisher    = {Curran Associates, Inc.},
	pages        = {1097--1105},
	editor       = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger}
}
@inproceedings{googlenet,
	title        = {Going Deeper with Convolutions},
	author       = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
	year         = 2014,
	month        = {June},
	booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition},
	address      = {Boston, MA},
	pages        = {1--9},
	doi          = {10.1109/CVPR.2015.7298594},
	issn         = {1063-6919},
	note         = {arXiv:1409.4842}
}
@inproceedings{orig-bert-2018,
	title        = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and Toutanova, Kristina},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
	address      = {Minneapolis, MN},
	pages        = {4171--4186},
	doi          = {10.18653/v1/N19-1423},
	url          = {https://www.aclweb.org/anthology/N19-1423}
}
@inproceedings{liu:2019,
	title        = {Text Summarization with Pretrained Encoders},
	author       = {Liu, Yang  and Lapata, Mirella},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	address      = {Hong Kong, China},
	pages        = {3730--3740},
	doi          = {10.18653/v1/D19-1387},
	url          = {https://www.aclweb.org/anthology/D19-1387}
}
@article{biobert,
	title        = {{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}},
	author       = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	year         = 2019,
	month        = {09},
	journal      = {Bioinformatics},
	volume       = 36,
	number       = 4,
	pages        = {1234--1240},
	doi          = {10.1093/bioinformatics/btz682},
	issn         = {1367-4803},
	url          = {https://doi.org/10.1093/bioinformatics/btz682},
	eprint       = {http://oup.prod.sis.lan/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btz682/30132027/btz682.pdf}
}
@article{lin-neural-recantation,
	title        = {Neural Hype, Justified! {A} recantation},
	author       = {Jimmy Lin},
	year         = 2019,
	journal      = {ACM SIGIR Forum},
	volume       = 53,
	url          = {http://sigir.org/wp-content/uploads/2019/december/p088.pdf},
	issue        = 2
}
@inproceedings{yang2019critically,
	title        = {Critically Examining the" Neural Hype" Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models},
	author       = {Yang, Wei and Lu, Kuang and Yang, Peilin and Lin, Jimmy},
	year         = 2019,
	booktitle    = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	address      = {Paris, France},
	pages        = {1129--1132},
	url          = {https://dl.acm.org/doi/10.1145/3331184.3331340},
	numpages     = 4,
	keywords     = {meta-analysis, document ranking, neural IR}
}
@article{roberta,
	title        = {{RoBERTa}: {A} Robustly Optimized {BERT} Pretraining Approach},
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = 2019,
	journal      = {Computing Research Repository},
	volume       = {abs/1907.11692},
	url          = {http://arxiv.org/abs/1907.11692},
	archiveprefix = {arXiv},
	eprint       = {1907.11692},
	timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1907-11692},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{drmm,
	title        = {A Deep Relevance Matching Model for Ad-hoc Retrieval},
	author       = {Jiafeng Guo and Yixing Fan and Qingyao Ai and Bruce Croft},
	year         = 2017,
	journal      = {Computing Research Repository},
	booktitle    = {CIKM},
	address      = {Indianapolis, IN},
	volume       = {abs/1711.08611},
	pages        = {55--64},
	url          = {http://arxiv.org/abs/1711.08611},
	archiveprefix = {arXiv},
	eprint       = {1711.08611},
	timestamp    = {Mon, 13 Aug 2018 16:46:36 +0200},
	biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1711-08611},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{abcnn,
	title        = {{ABCNN:} Attention-Based Convolutional Neural Network for Modeling Sentence Pairs},
	author       = {Wenpeng Yin and Hinrich Sch{\"{u}}tze and Bing Xiang and Bowen Zhou},
	year         = 2015,
	journal      = {Computing Research Repository},
	volume       = {abs/1512.05193},
	url          = {http://arxiv.org/abs/1512.05193},
	archiveprefix = {arXiv},
	eprint       = {1512.05193},
	timestamp    = {Mon, 13 Aug 2018 16:47:36 +0200},
	biburl       = {https://dblp.org/rec/bib/journals/corr/YinSXZ15},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{paccr,
	title        = {A Position-Aware Deep Model for Relevance Matching in Information Retrieval},
	author       = {Kai Hui and Andrew Yates and Klaus Berberich and Gerard de Melo},
	year         = 2017,
	journal      = {Computing Research Repository},
	volume       = {abs/1704.03940},
	url          = {http://arxiv.org/abs/1704.03940},
	archiveprefix = {arXiv},
	eprint       = {1704.03940},
	timestamp    = {Mon, 13 Aug 2018 16:47:06 +0200},
	biburl       = {https://dblp.org/rec/bib/journals/corr/HuiYBM17},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{reimers2019,
	title        = {Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks},
	author       = {Reimers, Nils  and Gurevych, Iryna},
	year         = 2019,
	month        = nov,
	booktitle    = {EMNLP},
	address      = {Hong Kong, China},
	pages        = {3982--3992},
	doi          = {10.18653/v1/D19-1410},
	url          = {https://www.aclweb.org/anthology/D19-1410.pdf}
}
@inproceedings{Armstrong:2009,
	title        = {Improvements That Don'T Add Up: Ad-hoc Retrieval Results Since 1998},
	author       = {Armstrong, T. and Moffat, A. and Webber, W. and Zobel, J.},
	year         = 2009,
	booktitle    = {CIKM},
	address      = {Hong Kong, China},
	pages        = {601--610},
	keywords     = {evaluation, retrieval experiment, survey, system measurement}
}
@article{search-like-an-expert-2022,
	title        = {Search like an expert: Reducing expertise disparity using a hybrid neural index for COVID-19 queries},
	author       = {Vincent Nguyen and Maciej Rybinski and Sarvnaz Karimi and Zhenchang Xing},
	year         = 2022,
	journal      = {Journal of Biomedical Informatics},
	volume       = 127,
	pages        = 104005,
	doi          = {https://doi.org/10.1016/j.jbi.2022.104005},
	issn         = {1532-0464},
	url          = {https://www.sciencedirect.com/science/article/pii/S1532046422000211},
	keywords     = {COVID-19, Universal sentence embeddings, Information retrieval, Natural language processing, Neural index, Dense retrieval, Medical misinformation, Biomedical search},
	abstract     = {Consumers from non-medical backgrounds often look for information regarding a specific medical information need; however, they are limited by their lack of medical knowledge and may not be able to find reputable resources. As a case study, we investigate reducing this knowledge barrier to allow consumers to achieve search effectiveness comparable to that of an expert, or a medical professional, for COVID-19 related questions. We introduce and evaluate a hybrid index model that allows a consumer to formulate queries using consumer language to find relevant answers to COVID-19 questions. Our aim is to reduce performance degradation between medical professional queries and those of a consumer. We use a universal sentence embedding model to project consumer queries into the same semantic space as professional queries. We then incorporate sentence embeddings into a search framework alongside an inverted index. Documents from this index are retrieved using a novel scoring function that considers sentence embeddings and BM25 scoring. We find that our framework alleviates the expertise disparity, which we validate using an additional set of crowdsourced—consumer—queries even in an unsupervised setting. We also propose an extension of our method, where the sentence encoder is optimised in a supervised setup. Our framework allows for a consumer to search using consumer queries to match the search performance with that of a professional.}
}

@inproceedings{opennir,
author = {MacAvaney, Sean},
title = {OpenNIR: A Complete Neural Ad-Hoc Ranking Pipeline},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371864},
doi = {10.1145/3336191.3371864},
abstract = {With the growing popularity of neural approaches for ad-hoc ranking, there is a need for tools that can effectively reproduce prior results and ease continued research by supporting current state-of-the-art approaches. Although several excellent neural ranking tools exist, none offer an easy end-to-end ad-hoc neural raking pipeline. A complete pipeline is particularly important for ad-hoc ranking because there are numerous parameter settings that have a considerable effect on the ultimate performance yet often are under-reported in current work (e.g., initial ranking settings, re-ranking threshold, training sampling strategy, etc.). In this work, I present a complete ad-hoc neural ranking pipeline which addresses these shortcomings: OpenNIR. The pipeline is easy to use (a single command will download required data, train, and evaluate a model), yet highly configurable, allowing for continued work in areas that are understudied. Aside from the core pipeline, the software also includes several bells and whistles that make use of components of the pipeline, such as performance benchmarking and tuning of unsupervised ranker parameters for fair comparisons against traditional baselines. The pipeline and these capabilities are demonstrated. The code is available, and contributions are welcome.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {845–848},
numpages = {4},
keywords = {reproducibility, ad-hoc ranking, neural ranking},
location = {Houston, TX, USA},
series = {WSDM '20}
}
